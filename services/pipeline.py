import os
import re
import json
import requests
from typing import List
from fastapi import HTTPException
from sklearn.metrics.pairwise import cosine_similarity

from config import GPT_MODEL, OPENAI_API_KEY, DB_CONFIG, BLOCKED_DOMAINS
from .db import fetch_user_preference_vector, fetch_message_embedding
from .utils import is_recommended
from schemas import AnalyzeResponse
from bs4 import BeautifulSoup
from urllib.parse import urlparse

from openai import OpenAI

client = OpenAI(api_key=OPENAI_API_KEY)

def openai_chat_completion(prompt: str) -> str:
    try:
        response = client.chat.completions.create(
            model=GPT_MODEL,
            messages=[{"role": "user", "content": prompt}],
            timeout=15
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print("â— openai_chat_completion ì‹¤íŒ¨:", e)
        raise e

def openai_embedding(text: str) -> List[float]:
    try:
        print("ğŸ”¹ openai_embedding í˜¸ì¶œ: ì…ë ¥ ê¸¸ì´ =", len(text))
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        vector = response.data[0].embedding
        short_vector = vector[:10] + (["..."] if len(vector) > 10 else [])
        print("ğŸ”¹ ì„ë² ë”© ê²°ê³¼ (ìš”ì•½):", short_vector)
        return vector
    except Exception as e:
        print("â— openai_embedding ì‹¤íŒ¨:", e)
        raise e


def extract_json_block(text: str) -> str:
    """
    LLM ì‘ë‹µì—ì„œ JSON ë¸”ë¡ë§Œ ì¶”ì¶œ
    """
    match = re.search(r'\{.*\}', text, re.DOTALL)
    if match:
        return match.group(0)
    else:
        raise ValueError("âš ï¸ JSON ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ì‘ë‹µ: " + text[:200])

def clarify_with_llm(message: str) -> dict:
    print("ğŸ”¹ clarify_with_llm ì‹œì‘:", message[:100])

    prompt = f"""
ë‹¤ìŒ ë©”ì‹œì§€ë¥¼ ì½ê³ , ì€ì–´ë‚˜ ì¤„ì„ë§, ê³ ìœ  ëª…ì‚¬ ë“± ë²¡í„° ì„ë² ë”© ì‹œ ì˜ë¯¸ê°€ ì œëŒ€ë¡œ í‘œí˜„ë˜ì§€ ì•Šì„ ìˆ˜ ìˆëŠ” ë‹¨ì–´ë“¤ì„ ìì—°ìŠ¤ëŸ½ê²Œ í’€ì–´ì„œ ì¨ì¤˜.
ì˜ˆ: "í”Œì " â†’ "í”„ë¡œì íŠ¸", "ê°“ìƒ" â†’ "ì—´ì‹¬íˆ ì‚¬ëŠ” ì‚¶", "ì¹´ê³µì¡±" â†’ "ì¹´í˜ì—ì„œ ê³µë¶€í•˜ëŠ” ì‚¬ëŒë“¤"

ë˜í•œ, ë©”ì‹œì§€ë¥¼ ì™„ì „íˆ ì´í•´í•˜ë ¤ë©´ ì•„ë˜ í•­ëª©ë“¤ì„ íŒë‹¨í•´ì¤˜:
1. ê³¼ê±° ë¬¸ë§¥(ëŒ€ëª…ì‚¬ë‚˜ ìƒëµëœ ë‚´ìš© ë“±)ì´ ê¼­ í•„ìš”í•œì§€
2. ì™¸ë¶€ ì •ë³´(ë‰´ìŠ¤, ìœ„í‚¤, ì‚¬ì „ ë“±)ê°€ í•„ìš”í•œì§€
3. ì´í›„ ê²€ìƒ‰ìš©ìœ¼ë¡œ ì ì ˆí•œ í•µì‹¬ í‚¤ì›Œë“œ (ìµœëŒ€ 3ê°œ, ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ì¤˜:
{{
  "clarified": "ìì—°ìŠ¤ëŸ½ê²Œ í’€ì–´ì„œ ì“´ ì „ì²´ ë©”ì‹œì§€",
  "needs_user_context": true/false,
  "db_keywords": [í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2],  # ë¬¸ë§¥ ê²€ìƒ‰ìš© í‚¤ì›Œë“œ
  "needs_external_info": true/false,
  "web_keywords": [í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2]  # ì™¸ë¶€ ê²€ìƒ‰ í‚¤ì›Œë“œ
}}

ë©”ì‹œì§€: "{message}"
"""

    try:
        raw_response = openai_chat_completion(prompt)
        print("ğŸ”¹ LLM ì‘ë‹µ ì›ë¬¸:", raw_response[:300])
        json_text = extract_json_block(raw_response)
        parsed = json.loads(json_text)

        return {
            "clarified": parsed.get("clarified", message),
            "needs_user_context": parsed.get("needs_user_context", False),
            "db_keywords": parsed.get("db_keywords", []),
            "needs_external_info": parsed.get("needs_external_info", False),
            "web_keywords": parsed.get("web_keywords", [])
        }

    except Exception as e:
        print("â— clarify_with_llm ì‹¤íŒ¨:", e)
        return {
            "clarified": message,
            "needs_user_context": False,
            "db_keywords": [],
            "needs_external_info": False,
            "web_keywords": []
        }

def fetch_user_messages_by_keywords(cochat_id: str, keywords: List[str]) -> List[str]:
    try:
        import psycopg2
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        keyword_clause = " OR ".join(["content ILIKE %s" for _ in keywords])
        params = [f"%{kw}%" for kw in keywords]
        query = f"""
            SELECT content FROM messages
            JOIN users ON messages.user_id = users.id
            WHERE users.cochat_id = %s AND ({keyword_clause})
            LIMIT 20;
        """
        cur.execute(query, (cochat_id, *params))
        rows = cur.fetchall()
        cur.close()
        conn.close()
        return [row[0] for row in rows]
    except Exception as e:
        print("â— DB ê²€ìƒ‰ ì‹¤íŒ¨:", e)
        return []

def search_user_db_context(content: str, user_contexts: List[str]) -> List[str]:
    if not user_contexts:
        return []
    query_vec = openai_embedding(content)
    context_vecs = [openai_embedding(txt) for txt in user_contexts]
    sims = cosine_similarity([query_vec], context_vecs)[0]
    top_k = sorted(range(len(sims)), key=lambda i: -sims[i])[:3]
    return [user_contexts[i] for i in top_k]

def clarify_with_rag(message: str, context_list: List[str]) -> str:
    context = "\n\n".join(context_list)
    prompt = f"ê³¼ê±° ë©”ì‹œì§€ë¥¼ ì°¸ê³ í•´ì„œ ì•„ë˜ ë¬¸ì¥ì„ ë” ëª…í™•í•˜ê²Œ í’€ì–´ ì¨ì£¼ì„¸ìš”:\n\"{message}\"\n\nê³¼ê±° ë©”ì‹œì§€:\n{context}"
    try:
        return openai_chat_completion(prompt)
    except Exception as e:
        print("â— clarify_with_rag ì‹¤íŒ¨:", e)
        return message
from urllib.parse import urlparse, parse_qs, unquote

def search_web_pages(query: str, max_results: int = 1) -> List[str]:
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        url = f"https://duckduckgo.com/html/?q={query}"
        r = requests.get(url, headers=headers, timeout=5)

        raw_links = re.findall(r'<a[^>]*class="result__a"[^>]*href="([^"]+)"', r.text)
        cleaned_links = []

        for raw_link in raw_links:
            # ì˜ˆ: //duckduckgo.com/l/?uddg=https%3A%2F%2Fexample.com
            if "uddg=" in raw_link:
                parsed_url = urlparse(raw_link)
                query_params = parse_qs(parsed_url.query)
                uddg = query_params.get("uddg")
                if uddg:
                    real_url = unquote(uddg[0])
                    if not any(blocked in real_url for blocked in BLOCKED_DOMAINS):
                        cleaned_links.append(real_url)
            else:
                # ê·¸ëƒ¥ ì¼ë°˜ì ì¸ ë§í¬ì¼ ê²½ìš° (ë“œë¬¼ê²Œ ë°œìƒ)
                if raw_link.startswith("//"):
                    real_url = "https:" + raw_link
                else:
                    real_url = raw_link
                if not any(blocked in real_url for blocked in BLOCKED_DOMAINS):
                    cleaned_links.append(real_url)

        return cleaned_links[:max_results]

    except Exception as e:
        print("â— search_web_pages ì‹¤íŒ¨:", e)
        return []


def extract_text_from_url(url: str) -> str:
    try:
        # ğŸ”¹ URL ë³´ì •: ìŠ¤í‚´ì´ ì—†ìœ¼ë©´ https:// ì¶”ê°€
        parsed = urlparse(url)
        if not parsed.scheme:
            if url.startswith("//"):
                url = "https:" + url
            else:
                url = "https://" + url

        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=headers, timeout=5)
        r.raise_for_status()  # ğŸ”¹ HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬

        soup = BeautifulSoup(r.content, "lxml")

        # ğŸ”¹ p íƒœê·¸ë§Œ ì¶”ì¶œ (ê¸¸ì´ ìˆëŠ” ê²ƒë§Œ)
        paragraphs = soup.find_all("p")
        text = "\n".join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20)

        if not text:
            print(f"âš ï¸ ë³¸ë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤: {url}")
        return text.strip()

    except requests.exceptions.RequestException as req_err:
        print(f"â— [ìš”ì²­ ì—ëŸ¬] URL ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {url}\n{req_err}")
    except Exception as e:
        print(f"â— [ê¸°íƒ€ ì—ëŸ¬] URL ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {url}\n{e}")

    return ""

def summarize_and_classify(message: str, context: str) -> dict:
    user_prompt = f"""
ë©”ì‹œì§€: {message}
ë¬¸ë§¥: {context}
JSON:
{{
  "summary": "...",
  "category": "..."
}}
"""
    system_prompt = """
ë„ˆëŠ” í•œêµ­ì–´ ë©”ì‹ ì € ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.
ì¹´í…Œê³ ë¦¬ëŠ”: "deadline", "payment", "public", "office", "others"
JSON í˜•ì‹ë§Œ ë°˜í™˜í•´:
{
  "summary": "...",
  "category": "..."
}
"""
    try:
        res = client.chat.completions.create(
            model=GPT_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )
        result = res.choices[0].message.content.strip()
        try:
            return json.loads(result)
        except:
            summary = re.search(r'"?summary"?\s*[:ï¼š]\s*"?([^"\n]+)"?', result)
            category = re.search(r'"?category"?\s*[:ï¼š]\s*"?([^"\n]+)"?', result)
            return {
                "summary": summary.group(1) if summary else "ìš”ì•½ ì‹¤íŒ¨",
                "category": category.group(1) if category else "others"
            }
    except Exception as e:
        print("â— ìš”ì•½/ë¶„ë¥˜ ì‹¤íŒ¨:", e)
        return {"summary": "ìš”ì•½ ì‹¤íŒ¨", "category": "others"}

def process_message_pipeline(message) -> AnalyzeResponse:
    try:
        original = message.content
        print(f"ğŸ”¹ Pipeline ì‹œì‘ - ì›ë³¸ ë©”ì‹œì§€ ê¸¸ì´: {len(original)}")

        # 1. ë©”ì‹œì§€ ëª…í™•í™”
        print("ğŸ”¹ clarify_with_llm í˜¸ì¶œ")
        gpt_result = clarify_with_llm(original)
        print("ğŸ”¹ clarify ê²°ê³¼:", gpt_result)

        clarified = gpt_result["clarified"]
        user_keywords = gpt_result.get("db_keywords", [])
        web_keywords = gpt_result.get("web_keywords", [])
        needs_user_context = gpt_result.get("needs_user_context", False)
        needs_external_info = gpt_result.get("needs_external_info", False)

        # 2. ì‚¬ìš©ì ë¬¸ë§¥ ì²˜ë¦¬
        if needs_user_context and user_keywords and message.cochat_id:
            print(f"ğŸ”¹ ì‚¬ìš©ì ë¬¸ë§¥ í•„ìš” - í‚¤ì›Œë“œ: {user_keywords}")
            user_msgs = fetch_user_messages_by_keywords(message.cochat_id, user_keywords)
            print(f"ğŸ”¹ ì‚¬ìš©ì ë©”ì‹œì§€ ìˆ˜: {len(user_msgs)}")
            context = search_user_db_context(clarified, user_msgs)
            print(f"ğŸ”¹ ìœ ì‚¬ë„ ë†’ì€ ë¬¸ë§¥ ìˆ˜: {len(context)}")
            clarified = clarify_with_rag(clarified, context)
            print("ğŸ”¹ ì¬ëª…í™•í™” ì™„ë£Œ")

        # 3. ì™¸ë¶€ ì •ë³´ ê²€ìƒ‰
        if needs_external_info and web_keywords:
            print(f"ğŸ”¹ ì™¸ë¶€ ì •ë³´ í•„ìš” - í‚¤ì›Œë“œ: {web_keywords}")
            for kw in web_keywords:
                urls = search_web_pages(kw)
                print(f"ğŸ”¹ ê²€ìƒ‰ëœ URLë“¤: {urls}")
                for url in urls:
                    text = extract_text_from_url(url)
                    if text:
                        print(f"ğŸ”¹ URLì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {url}")
                        break

        # 4. ìš”ì•½ ë° ë¶„ë¥˜
        context_text = ""  # í˜„ì¬ contextëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        print("ğŸ”¹ summarize_and_classify í˜¸ì¶œ")
        metadata = summarize_and_classify(clarified, context_text)
        print("ğŸ”¹ ìš”ì•½/ë¶„ë¥˜ ê²°ê³¼:", metadata)

        # 5. ë©”ì‹œì§€ ì„ë² ë”©
        print("ğŸ”¹ openai_embedding í˜¸ì¶œ (clarified ë©”ì‹œì§€)")
        message_vector = openai_embedding(clarified)
        print("ğŸ”¹ ë©”ì‹œì§€ ë²¡í„° (ì• 10ê°œ):", message_vector[:10], "..." if len(message_vector) > 10 else "")

        # 6. ì‚¬ìš©ì ë²¡í„°ì™€ ì¶”ì²œ ì—¬ë¶€
        user_vector = fetch_user_preference_vector(message.cochat_id) if message.cochat_id else []
        print("ğŸ”¹ ì‚¬ìš©ì ë²¡í„° (ì• 10ê°œ):", user_vector[:10], "..." if len(user_vector) > 10 else "")
        recommended = is_recommended(user_vector, message_vector)
        print("ğŸ”¹ ì¶”ì²œ ì—¬ë¶€:", recommended)

        # 7. ìš”ì•½ ì‹¤íŒ¨ ì‹œ fallback ì²˜ë¦¬
        if metadata.get("summary") == "ìš”ì•½ ì‹¤íŒ¨" and recommended:
            short = clarified[:50] + "..." if len(clarified) > 50 else clarified
            fallback = {
                "deadline": f"ê¸°í•œ ê´€ë ¨ ë©”ì‹œì§€: {short}",
                "payment": f"ê²°ì œ ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤: {short}",
                "public": f"ê³µì§€ ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤: {short}",
                "office": f"ì‚¬ë¬´ì‹¤ ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤: {short}",
                "others": f"ê¸°íƒ€ ë©”ì‹œì§€ì…ë‹ˆë‹¤: {short}"
            }.get(metadata.get("category", "others"), short)
            metadata["summary"] = fallback
            print("ğŸ”¹ ìš”ì•½ ì‹¤íŒ¨ fallback ì‚¬ìš©:", fallback)

        print("âœ… Pipeline ì™„ë£Œ")
        return AnalyzeResponse(
            content=original,
            clarified=clarified,
            summary=metadata.get("summary", "ìš”ì•½ ì‹¤íŒ¨"),
            category=metadata.get("category", "others"),
            embedding_vector=message_vector,
            recommended=recommended
        )

    except Exception as e:
        print("â— ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨:", e)
        raise e

def create_embedding(req):
    embedding = openai_embedding(req.text)
    return {"embedding": embedding, "dim": len(embedding)}

def update_preference_by_message(req):
    message_vec = fetch_message_embedding(req.message_id)
    if not message_vec:
        raise HTTPException(status_code=404, detail="Message embedding not found")

    user_vec = fetch_user_preference_vector(req.cochat_id)
    if not user_vec:
        return {
            "status": "no existing vector, returning message vector",
            "updated_vector": message_vec,
            "vector_length": len(message_vec)
        }

    updated = [(u + m) / 2 for u, m in zip(user_vec, message_vec)]
    return {
        "status": "vector calculated (not saved)",
        "updated_vector": updated,
        "vector_length": len(updated)
    }